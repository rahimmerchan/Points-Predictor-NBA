{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d53f44f",
   "metadata": {},
   "source": [
    "# NBA Yearly Player Stats Web Scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee61522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a8395",
   "metadata": {},
   "source": [
    "#### Web Scrape NBA players data\n",
    "\n",
    "Using NBA reference, we will web scrape, NBA players stats from 1991-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f79b5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a players folder \n",
    "folder_name = \"players\"\n",
    "\n",
    "#checks if the folder exist \n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(f\"Folder '{folder_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04432be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base url for nba reference players data\n",
    "base_url = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\"\n",
    "years = range(1991, 2021) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4759206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through each year from 1991 to 2020\n",
    "for year in years:\n",
    "    #gets the url for the current year\n",
    "    url = base_url.format(year)\n",
    "    #Send an HTTP GET request to the URL to retrieve the web page content\n",
    "    data = requests.get(url)\n",
    "    #save the web page content to the players folder\n",
    "    with open(\"players/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae399b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "# Loop through each year from 1991 to 2020\n",
    "for year in years:\n",
    "    # Read the HTML content of the web page for the current year\n",
    "    with open(\"players/{}.html\".format(year)) as f:\n",
    "        page = f.read()\n",
    "\n",
    "    #Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    # Check if the target element exists\n",
    "    thead_element = soup.find('tr', class_=\"thead\")\n",
    "    if thead_element:\n",
    "        thead_element.decompose()\n",
    "    else:\n",
    "        print(f\"Element not found for year {year}, skipping...\")\n",
    "        continue\n",
    "    #gets a df of each year and appends it in the dfs list\n",
    "    player_table = soup.find_all(id=\"per_game_stats\")[0]\n",
    "    player_df = pd.read_html(str(player_table))[0]\n",
    "    player_df[\"Year\"] = year\n",
    "    dfs.append(player_df)\n",
    "\n",
    "# Concatenate the dataframes from different years\n",
    "old_players_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f71e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves the old_players_df to csv\n",
    "old_players_df.to_csv(\"old_players.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb693768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the players df from 1991-2020\n",
    "old_players_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da19fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a players_new folder \n",
    "import os\n",
    "\n",
    "folder_name = \"players_new\"\n",
    "\n",
    "#checks if the folder exist \n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(f\"Folder '{folder_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdc17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base url for nba reference players data\n",
    "base_url = \"https://www.basketball-reference.com/leagues/NBA_{}_per_game.html\"\n",
    "years = range(2021,2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through each year from 2021 to 2023\n",
    "for year in years:\n",
    "    #gets the url for the current year\n",
    "    url = base_url.format(year)\n",
    "    #Send an HTTP GET request to the URL to retrieve the web page content\n",
    "    data = requests.get(url)\n",
    "    #save the web page content to the players folder\n",
    "    with open(\"players_new/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd6348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "# Loop through each year from 1991 to 2020\n",
    "for year in years:\n",
    "    # Read the HTML content of the web page for the current year\n",
    "    with open(\"players_new/{}.html\".format(year)) as f:\n",
    "        page = f.read()\n",
    "\n",
    "    #Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "    # Check if the target element exists\n",
    "    thead_element = soup.find('tr', class_=\"thead\")\n",
    "    if thead_element:\n",
    "        thead_element.decompose()\n",
    "    else:\n",
    "        print(f\"Element not found for year {year}, skipping...\")\n",
    "        continue\n",
    "    #gets a df of each year and appends it in the dfs list\n",
    "    player_table = soup.find_all(id=\"per_game_stats\")[0]\n",
    "    player_df = pd.read_html(str(player_table))[0]\n",
    "    player_df[\"Year\"] = year\n",
    "    dfs.append(player_df)\n",
    "\n",
    "# Concatenate the dataframes from different years\n",
    "new_players_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f6108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the new_players df\n",
    "new_players_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f69c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it to a csv file\n",
    "new_players_df.to_csv(\"new_players_yearly_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6323199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatentates the old_players_df and new_players_df\n",
    "full_players_df= pd.concat([old_players_df, new_players_df], ignore_index=True)\n",
    "full_players_df.to_csv(\"players.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbb66ca",
   "metadata": {},
   "source": [
    "#### Web Scrape NBA Teams data\n",
    "\n",
    "Using NBA reference, we will web scrape, NBA teams stats from 1991-2023 for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712209c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Create a teams_old folder \n",
    "folder_name = \"teams_old\"\n",
    "\n",
    "#checks if the folder exists\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(f\"Folder '{folder_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17003be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base url for nba reference team data\n",
    "team_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_standings.html\"\n",
    "years = range(1991, 2021) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92b25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through each year from 1991 to 2020\n",
    "for year in years:\n",
    "    #gets the url for the current year\n",
    "    url = team_stats_url.format(year)\n",
    "    #Send an HTTP GET request to the URL to retrieve the web page content\n",
    "    data = requests.get(url)\n",
    "    #save the web page content to the players folder\n",
    "    with open(\"teams_old/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "#loop through each year \n",
    "for year in years:\n",
    "    # Read the HTML content of the web page for the current year\n",
    "    with open(\"teams_old/{}.html\".format(year)) as f:\n",
    "        page = f.read()\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    #append all the eastern conference data to dfs list\n",
    "    soup.find('tr', class_=\"thead\").decompose()\n",
    "    e_table = soup.find_all(id=\"divs_standings_E\")[0]\n",
    "    e_df = pd.read_html(str(e_table))[0]\n",
    "    e_df[\"Year\"] = year\n",
    "    e_df[\"Team\"] = e_df[\"Eastern Conference\"]\n",
    "    del e_df[\"Eastern Conference\"]\n",
    "    dfs.append(e_df)\n",
    "    \n",
    "    #append all the western conference data to dfs list\n",
    "    w_table = soup.find_all(id=\"divs_standings_W\")[0]\n",
    "    w_df = pd.read_html(str(w_table))[0]\n",
    "    w_df[\"Year\"] = year\n",
    "    w_df[\"Team\"] = w_df[\"Western Conference\"]\n",
    "    del w_df[\"Western Conference\"]\n",
    "    dfs.append(w_df)\n",
    "\n",
    "#concatentate all eastern and western conference dfs\n",
    "teams_old = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save it to a csv file\n",
    "teams_old.to_csv(\"old_teams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb8fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base url for nba reference team data\n",
    "team_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_standings.html\"\n",
    "years = range(2021, 2024) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e9cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Create a teams_new folder \n",
    "folder_name = \"teams_new\"\n",
    "\n",
    "#checks if it exists\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(f\"Folder '{folder_name}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through each year from 2021 to 2023\n",
    "for year in years:\n",
    "    #gets the url for the current year\n",
    "    url = team_stats_url.format(year)\n",
    "    #Send an HTTP GET request to the URL to retrieve the web page content\n",
    "    data = requests.get(url)\n",
    "    #save the web page content to the players folder\n",
    "    with open(\"teams_new/{}.html\".format(year), \"w+\") as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ce58a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "#loop through each year \n",
    "for year in years:\n",
    "    # Read the HTML content of the web page for the current year\n",
    "    with open(\"teams_old/{}.html\".format(year)) as f:\n",
    "        page = f.read()\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    #append all the eastern conference data to dfs list\n",
    "    soup.find('tr', class_=\"thead\").decompose()\n",
    "    e_table = soup.find_all(id=\"divs_standings_E\")[0]\n",
    "    e_df = pd.read_html(str(e_table))[0]\n",
    "    e_df[\"Year\"] = year\n",
    "    e_df[\"Team\"] = e_df[\"Eastern Conference\"]\n",
    "    del e_df[\"Eastern Conference\"]\n",
    "    dfs.append(e_df)\n",
    "    \n",
    "    #append all the western conference data to dfs list\n",
    "    w_table = soup.find_all(id=\"divs_standings_W\")[0]\n",
    "    w_df = pd.read_html(str(w_table))[0]\n",
    "    w_df[\"Year\"] = year\n",
    "    w_df[\"Team\"] = w_df[\"Western Conference\"]\n",
    "    del w_df[\"Western Conference\"]\n",
    "    dfs.append(w_df)\n",
    "\n",
    "#concatentate all eastern and western conference dfs\n",
    "teams_new = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5abce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves it to a csv file\n",
    "teams_new.to_csv(\"new_teams.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbce33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatentate both dfs \n",
    "full_teams_df= pd.concat([teams_old, teams_new], ignore_index=True)\n",
    "full_teams_df.to_csv(\"teams_record.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085571df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calls the full_teams_df\n",
    "full_teams_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
